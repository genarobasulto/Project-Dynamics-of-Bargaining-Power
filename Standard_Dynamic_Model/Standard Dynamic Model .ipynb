{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Dynamics of Bargaining Power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Dynamic Model \n",
    "\n",
    "#### Genaro, Itza, & Sonia\n",
    "#### May 2021 \n",
    "\n",
    "This code was made specifically to solve the dynamyc single objective model of principal-agent proposed in Wang. \n",
    "\n",
    "This aplication uses the JuMP library and the Ipopt solver to solve the optimization problrem given a value for the bargaining power.\n",
    "\n",
    "The use uf this library allows us to easily change the parametrization of the problem and rely on a robust solver to find solutions in multi-criteria non-linear restricted optimization problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using JuMP, Ipopt # Loading optimization libraries.\n",
    "                  # Ipopt solve a nonlinear optimization problem.\n",
    "using DataFrames  # Loading frame libraries.\n",
    "using ExcelFiles, XLSX  # Loading excel files libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#v(c,a,g=1,alpha=1)=-exp(g*(a-alpha*c))  # Declaration of v (agent utility function); \n",
    "                                        # g is the coefficient of risk aversion\n",
    "                                        # alpha is a cost coefficientv\n",
    "\n",
    "v(c, a, h=0.75) =  c^(1-h)/(1-h)- a^2  #CRA Function \n",
    "                                    #h is a oefficient of risk aversion\n",
    "                                    # a is the agent effort\n",
    "\n",
    "u(y,w)=y-w  #Declaration of the principal utility function \n",
    "            #y is the current output\n",
    "            #w is the salary paid to the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables and Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [0.1 0.2]    # Actions set [al ah]\n",
    "Y = [0.4 0.8]    # Outcomes set [yl yh]\n",
    "f = [2/3 1/3; 1/3 2/3]    # Probability matrix  [yh|ah yl|ah; yh|al yl|al]\n",
    "beta = 0.96    # Future discount factor\n",
    "N = 100    # Number of intervals for the state variable\n",
    "Max_iter = 250 #Max number of iterations \n",
    "digits_tol = 3 #Digits of precision of the solution  \n",
    "VFH=zeros(100)\n",
    "VFL=zeros(100)\n",
    "for k in K\n",
    "    VFH[k]=90\n",
    "    VFL[k]=19\n",
    "end\n",
    "iter=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Effort Model Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function Model_High_Effort(k, P, Q, h, V, U0)\n",
    "    \"\"\"\n",
    "    This functions declares and solves the High effort model for the principal utility\n",
    "    Returns optimal values if found \n",
    "    \"\"\"\n",
    "    model_high_effort = Model(with_optimizer(Ipopt.Optimizer, tol = 1e-7, max_iter = 1000, print_level=1)) \n",
    "    # A model to incentivize high effort is created specifying the optimization method\n",
    "\n",
    "    #variables del modelo xl=wl, xh=wh\n",
    "    @variable(model_high_effort, xl>=0.00001) \n",
    "    @variable(model_high_effort, xh>=0.00001)\n",
    "\n",
    "    #Declare the agent's utility function within the model.\n",
    "\n",
    "    register(model_high_effort, :v, 3, v, autodiff=true) # model_high_effort, the model name\n",
    "                                            # :v,  **********                                          \n",
    "                                            # 2, number of variables\n",
    "                                            # v, declare the agent's utility function \n",
    "                                            # autodiff,    **********\n",
    "    \n",
    "    register(model_high_effort, :u, 2, u, autodiff=true) # model_high_effort, the model name\n",
    "                                            # :u,  **********                                          \n",
    "                                            # 2, number of variables\n",
    "                                            # u, declare the principal's utility function \n",
    "                                            # autodiff,    **********\n",
    "\n",
    "    #Non-linear type expressions are declared, the expected utility of the agent, within the model.\n",
    "    EV_H = @NLexpression(model_high_effort, f[1,1]*(v(xh,A[2],h)+beta*V[P])+f[1,2]*(v(xl,A[2],h)+beta*V[Q])) #E(V|ah)\n",
    "    EV_L = @NLexpression(model_high_effort, f[2,1]*(v(xh,A[1],h)+beta*V[P])+f[2,2]*(v(xl,A[1],h)+beta*V[Q])) #E(V|al)\n",
    "    EU_H = @NLexpression(model_high_effort,f[1,1]*(u(Y[2],xh)+beta*U0[P])+f[1,2]*(u(Y[1],xl)+beta*U0[Q])) #E(U|ah)\n",
    "    \n",
    "    #Objective function; expected utility of the principal given ah.\n",
    "    @NLobjective(model_high_effort, Max, EU_H) \n",
    "\n",
    "    @constraint(model_high_effort, xh<=Y[2]) #Financial capacity restriction for high compensation; wh<=yh.\n",
    "    @constraint(model_high_effort, xl<=Y[1])  #Financial capacity restriction for low compensation; wl<=yl.\n",
    "    @NLconstraint(model_high_effort, EV_H>=EV_L) #Incentive constraint to incentivize ah.\n",
    "    @NLconstraint(model_high_effort, EV_H == V[k]) #Participation constraint to incentivize ah. \n",
    "\n",
    "    #The problem posed for incentive ah is solved.\n",
    "    JuMP.optimize!(model_high_effort) \n",
    "    \n",
    "    # The optimal value of the function is saved given ah.\n",
    "    uh = getobjectivevalue(model_high_effort) \n",
    "    stat = termination_status(model_high_effort)\n",
    "    wl = value(xl)\n",
    "    wh = value(xh)\n",
    "    \n",
    "    return uh, wl, wh, stat\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Effort Model Specification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function Model_Low_Effort(k,P,Q,h, V, U0)\n",
    "    \"\"\"\n",
    "    This functions declares and solves the Low effort model for the principal utility\n",
    "    Returns optimal values if found \n",
    "    \"\"\"\n",
    "    \n",
    "    # A optimization model is created to incentive al.\n",
    "    model_low_effort = Model(with_optimizer(Ipopt.Optimizer, tol = 1e-7, max_iter = 1000, print_level=1)) \n",
    "\n",
    "    #Declare the decision variables for the model: xl = wl, xh = wh.\n",
    "    @variable(model_low_effort, xl>=0.00001) \n",
    "    @variable(model_low_effort, xh>=0.00001)\n",
    "\n",
    "    #Declare the agent's utility function within the model.\n",
    "    register(model_low_effort, :v, 3, v, autodiff=true) # model_low_effort, the model name\n",
    "                                            # :v,  **********                                          \n",
    "                                            # 3, number of variables\n",
    "                                            # v, declare the agent's utility function \n",
    "                                            # autodiff,    **********\n",
    "    register(model_low_effort, :u, 2, u, autodiff=true)# model_low_effort, the model name\n",
    "                                            # :u,  **********                                          \n",
    "                                            # 2, number of variables\n",
    "                                            # u, declare the principal's utility function \n",
    "                                            # autodiff,    **********\n",
    "    \n",
    "    # Non-linear type expressions are declared, the expected utility of the agent, within the model.\n",
    "    \n",
    "    EV_H = @NLexpression(model_low_effort, f[1,1]*(v(xh,A[2],h)+beta*V[P])+f[1,2]*(v(xl,A[2],h)+beta*V[Q])) #E(V|ah)\n",
    "    EV_L = @NLexpression(model_low_effort, f[2,1]*(v(xh,A[1],h)+beta*V[P])+f[2,2]*(v(xl,A[1],h)+beta*V[Q])) #E(V|al)\n",
    "    EU_L = @NLexpression(model_low_effort,f[2,1]*(u(Y[2],xh)+beta*U0[P])+f[2,2]*(u(Y[1],xl)+beta*U0[Q])) # E(u|al))\n",
    "    \n",
    "    # Objective function; expected utility of the principal given al.l\n",
    "    @NLobjective(model_low_effort, Max, EU_L) \n",
    "\n",
    "    @constraint(model_low_effort, xh <= Y[2]) # Financial capacity restriction for high compensation; wh<=yh.\n",
    "    @constraint(model_low_effort, xl <= Y[1]) # Financial capacity restriction for low compensation; wl<=yl.\n",
    "    \n",
    "    @NLconstraint(model_low_effort, EV_H <= EV_L)  # Incentive constraint to incentivize al.\n",
    "    @NLconstraint(model_low_effort, EV_L == V[k])  # Participation constraint to incentivize al.\n",
    "\n",
    "    # The problem posed for incentive al is solved.\n",
    "    JuMP.optimize!(model_low_effort)\n",
    "    \n",
    "    # The optimal value of the function is saved given al.\n",
    "    ul=getobjectivevalue(model_low_effort) #Se guarda el valor optimo de la funcion para al\n",
    "    stat = termination_status(model_low_effort)\n",
    "    wl = value(xl)\n",
    "    wh = value(xh)\n",
    "    \n",
    "    return ul, wl, wh, stat\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feasible Indexes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function feasible_Indexes(h, V, U0)\n",
    "    \"\"\"\n",
    "    This functions finds the indexes where the principal optimization problem has a feasible solution.\n",
    "    Returns array of feasible indexes. \n",
    "    \"\"\"\n",
    "    K0 = 1:10 #Initial guess of possible feasible indexes\n",
    "    KN = 1:100 #Final set of possible feasible indexes\n",
    "    \n",
    "    #loop while the initial set and the final set of indexes are different \n",
    "    while K0 != KN \n",
    "    \n",
    "        K0 = KN \n",
    "        KN = []\n",
    "\n",
    "        for k in K0 #loop for each index in the set\n",
    "\n",
    "            fact = 0 #factibility indicator \n",
    "\n",
    "            for P in k:min(K0[end], Int(VFH[k])+3) # Loop for comparison value 1\n",
    "\n",
    "                for Q in max(K0[1],Int(VFL[k]-3)):k # Loop for comparison value 2\n",
    "\n",
    "                    stat_high = Model_High_Effort(k, P, Q, h, V, U0)[4] #Solve the model for high effort, return termination status\n",
    "        \n",
    "                    if (stat_high == MOI.LOCALLY_SOLVED) #check if the model for high effort on index k if feasible, if so exit the loopfor comprasion value 2 \n",
    "                        fact=1 \n",
    "                        break    \n",
    "                    end\n",
    "\n",
    "                    stat_low = Model_Low_Effort(k,P,Q, h, V, U0)[4] #Solve the model for low effort, return termination status\n",
    "\n",
    "                    if (stat_low == MOI.LOCALLY_SOLVED) #check if the model for low effort on index k if feasible, if so exit the loopfor comprasion value 2 \n",
    "                        fact = 1\n",
    "                        break\n",
    "                    end\n",
    "\n",
    "                end\n",
    "                \n",
    "                #if the index is feasible, break the loop for comprasion value 1 \n",
    "                if fact == 1 \n",
    "                    break\n",
    "                end\n",
    "\n",
    "            end\n",
    "            \n",
    "            #if the index is feasible, append it on the final set\n",
    "            if fact == 1\n",
    "                append!(KN, k)\n",
    "            end\n",
    "\n",
    "        end\n",
    "\n",
    "        print(KN, \"\\n\")\n",
    "\n",
    "    end\n",
    "    \n",
    "    #Return the final set of feasible indexes\n",
    "    return KN \n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function Dynamic_Pareto_Frontier(K, V, U0, UF, digits_tol, Max_iter, h)\n",
    "    \"\"\"\n",
    "    This functions finds the stationary point of the Bellman Equation for\n",
    "    the standard dynamic model.\n",
    "    Returns optimal values if found \n",
    "    \"\"\"\n",
    "    \n",
    "    iter=0 #Current number of method iteration \n",
    "    size_iter = length(K) #Size of the iteration \n",
    "    AcOpt = [] # List of optimal actions for K\n",
    "    WHOpt = [] # List of hight optimal compensation for K\n",
    "    WLOpt = [] # List of low optimal compensation for K\n",
    "    VFH=zeros(100) #List of promised discounted utility for the principal if yh occurs.\n",
    "    VFL=zeros(100)# List of promised discounted utility for the principal if yl occurs.\n",
    "    for k in K\n",
    "        VFH[k]=K[end]\n",
    "        VFL[k]=K[1]\n",
    "    end\n",
    "    #loop while the initial value for the state variable is different from it's final value. \n",
    "    while round.(U0[K],digits = digits_tol)!=round.(UF[K],digits = digits_tol)\n",
    "    \n",
    "        AcOpt=[] #List of recomended actions \n",
    "        WHOpt=[] #List of compensations if yh occurs\n",
    "        WLOpt=[] #List of compensations if yl occurs \n",
    "        U0[K]=UF[K] #Update state variable\n",
    "        UF=zeros(100) #Final value of the state variable \n",
    "\n",
    "        for k in K # For each k in the feasible index set\n",
    "\n",
    "            uval=0 #Value of the principal's current discounted utility \n",
    "            wl=0 #Low compensation \n",
    "            wh=0 #High compendation \n",
    "            accopt=0 #Recomended action \n",
    "            p=0 #Optimal comprasion value 1 \n",
    "            q=0 #Optimal comprasion value 2 \n",
    " \n",
    "            for P in k:min(K[end], Int(VFH[k])+3) # Loop for comparison value 1\n",
    "\n",
    "                for Q in max(K[1],Int(VFL[k]-3)):k # Loop for comparison value 2\n",
    "\n",
    "                    uh, xl, xh, stat_high = Model_High_Effort(k, P, Q, h, V, U0) #High Effort Model is Solved\n",
    "                    \n",
    "                    # If the principal's utility with the high effort action is higher than 0 and the model is feasible update the state variables\n",
    "                    if (uh>uval) & (stat_high == MOI.LOCALLY_SOLVED)\n",
    "                        uval=uh\n",
    "                        wh=xh\n",
    "                        wl=xl\n",
    "                        p=P\n",
    "                        q=Q\n",
    "                        accopt=A[2]\n",
    "                       \n",
    "                    end\n",
    "\n",
    "                    ul, xl, xh, stat_low = Model_Low_Effort(k, P, Q, h, V, U0)\n",
    "                    \n",
    "                    # If the principal's utility with the low effort action is higher than with the low effort and the model is feasible update the state variables\n",
    "                    if (ul>uval) & (stat_low == MOI.LOCALLY_SOLVED)\n",
    "                        uval=ul\n",
    "                        wh=xh\n",
    "                        wl=xl\n",
    "                        p=P\n",
    "                        q=Q\n",
    "                        accopt=A[1]\n",
    "                \n",
    "                    end\n",
    "\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            #The new values for the state variables are saved\n",
    "            UF[k]=uval \n",
    "            append!(WHOpt, wh) \n",
    "            append!(WLOpt,wl)\n",
    "            append!(AcOpt, accopt)\n",
    "            VFH[k]=p\n",
    "            VFL[k]=q\n",
    "            \n",
    "            if iter<=1 \n",
    "                print(\"K= \", k, \"  ul=\", uval,\"   wh=\", wh, \"   wl=\", wl, \"  P=\",p,\"  Q=\",q,\"  \", accopt, \"\\n\")\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        iter+=1 #Update number of iterations \n",
    "        println(iter)\n",
    "        \n",
    "        if iter == Max_iter #if the number of iterations is higher than the Max-iter, stationary point could not be found \n",
    "            break\n",
    "        end\n",
    "\n",
    "    end\n",
    "    \n",
    "    #Creation of the DataFrame\n",
    "    DataEstPoint = DataFrame(\n",
    "    K=K,\n",
    "    Utilidad_Agente=V[K],\n",
    "    Utilidad_Principal=U0[K],\n",
    "    Compensacion_YH=WHOpt,\n",
    "    Compensacion_YL=WLOpt,\n",
    "    Accion_Recomendada=AcOpt, \n",
    "    VFH=VFH[K], \n",
    "    VFL=VFL[K])\n",
    "    \n",
    "    return DataEstPoint\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop for diferent values of h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "H = [0.44,0.45,0.46,0.47,0.48,0.49]\n",
    "for h in H  \n",
    "    vl = v(0,A[2], h)/(1-beta) # State variable`s lower limit \n",
    "    vh = ((1/3)*v(Y[1],A[1],h)+(2/3)*v(Y[2],A[1],h))/(1-beta)  # State variable's upper limit\n",
    "    V = LinRange(vl, vh, N) # State variable's vector \n",
    "    UF = zeros(100) # List of final values of the principal utility\n",
    "    Din = DataFrame(XLSX.readtable(string(\"DataEstPointh\",repr(round.(h-0.01,digits = 2)),\".xlsx\"), \"Sheet1\")...)\n",
    "    K_last = Din[:,\"K\"]\n",
    "    UF[K_last] = Din[:,\"Utilidad_Principal\"]\n",
    "    U0 = ones(100) # State variable's initial value\n",
    "    K =  feasible_Indexes(h, V, U0) #Find feasible indexes for a given value of h \n",
    "    DataEstPoint = Dynamic_Pareto_Frontier(K, V, U0, UF, digits_tol, Max_iter, h) #Run the routine to find the stationary point\n",
    "    save(string(\"DataEstPointh\",repr(h),\".xlsx\"), DataEstPoint) #Save the dataframe to an excel file. \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animation Plot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Plots, Plots.PlotMeasures    # Plots packages load\n",
    "using DataFrames\n",
    "using ExcelFiles\n",
    "using XLSX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pareto Frontier Animation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theme(:ggplot2)     # Graphics theme\n",
    "\n",
    "anim = @animate for h_now = 10:99\n",
    "    Data_h = DataFrame(XLSX.readtable(string(\"DataEstPointh\",repr.(round(h_now/100,digits = 2)),\".xlsx\"), \"Sheet1\")...)\n",
    "    Principal_Utilities = Data_h[4:end,\"Utilidad_Principal\"]\n",
    "    K = Data_h[4:end,\"K\"]\n",
    "    plot(K, Principal_Utilities, \n",
    "    xlabel = \"Agent reservation utility, level k\",\n",
    "    ylabel =  \"Principal's optimal expected utility\",\n",
    "    title=\"Pareto Frontier for different levels of h.\",\n",
    "    labels = string(\"h=\",repr(h_now/100)),\n",
    "    legend = :topright,\n",
    "    ylims=(0,20), \n",
    "    xlims=(0,100))\n",
    "    \n",
    "end\n",
    " \n",
    "gif(anim, \"Principal_Utilities_varying_h_Dynamic.gif\", fps = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compensation Scheme Animation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theme(:ggplot2)     # Graphics theme\n",
    "\n",
    "anim = @animate for h_now = 10:99\n",
    "    Data_h = DataFrame(XLSX.readtable(string(\"DataEstPointh\",repr.(round(h_now/100,digits = 2)),\".xlsx\"), \"Sheet1\")...)\n",
    "    High_Compensations = Data_h[4:end,\"Compensacion_YH\"]\n",
    "    Low_Compensations = Data_h[4:end,\"Compensacion_YL\"]\n",
    "    K = Data_h[4:end,\"K\"]\n",
    "    scatter(K, [Low_Compensations, High_Compensations], \n",
    "    shape=[:hline :+ :utri],\n",
    "    markersize=8,\n",
    "    title=\"Compensation for different levels of h.\", \n",
    "    legend=:topleft,\n",
    "    label=[string(\"WL, h=\",repr(h_now/100)) string(\"Wh, h=\",repr(h_now/100)) ],\n",
    "    xlabel = \"Agent reservation utility, level k\",\n",
    "    ylabel=\"Compensation\",\n",
    "    ylims=(0,0.8), \n",
    "    xlims=(10,100))\n",
    "    \n",
    "end\n",
    " \n",
    "gif(anim, \"Dynamic_Compensation_varying_h.gif\", fps = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theme(:ggplot2)     # Graphics theme\n",
    "\n",
    "anim = @animate for h_now = 10:99\n",
    "    Data_h = DataFrame(XLSX.readtable(string(\"DataEstPointh\",repr.(round(h_now/100,digits = 2)),\".xlsx\"), \"Sheet1\")...)\n",
    "    VFH = Data_h[4:end,\"VFH\"]\n",
    "    VFL = Data_h[4:end,\"VFL\"]\n",
    "    K = Data_h[4:end,\"K\"]\n",
    "    scatter(K, [VFL, VFH], \n",
    "    shape=[:hline :+ :utri],\n",
    "    markersize=8,\n",
    "    title=\"Promised Discounted Utility for different levels of h.\", \n",
    "    legend=:topleft,\n",
    "    label=[string(\"VFL, h=\",repr(h_now/100)) string(\"VFH, h=\",repr(h_now/100)) ],\n",
    "    xlabel = \"Agent reservation utility, level k\",\n",
    "    ylabel=\"Promised Discounted Utility\",\n",
    "    ylims=(0,100), \n",
    "    xlims=(0,100))\n",
    "    \n",
    "end\n",
    " \n",
    "gif(anim, \"Promised_Utility_varying_h.gif\", fps = 5)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
